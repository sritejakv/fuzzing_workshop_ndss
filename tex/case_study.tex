\section{Case Studies}
\label{sec:case_studies}

\begin{itemize}
    \item Fuzzers reach saturation at some point which means there are not more coverage gains seed after this time point. So, in order to enable fair comparison we only consider the latest timepoint in which any of the evaluated fuzzers produce coverage gain as the end time of the fuzzing run. Such an assumption also does not add the unnecessary event sequences during the evaluation.
    \item Configurations of all experiments: fuzzbench, number of trials, trial runtime, etc.
\end{itemize}

We believe that there is no single fuzzer that can provide better performance across multiple target programs.
Hence, in our evaluation, we say that fuzzer A is better than fuzzer B, when fuzzer A provides better performance on more number of targets than fuzzer B in our benchmark.

In the following, we discuss three case studies which evaluate fuzzers using our proposed metrics (\sectionsubref{sec:fuzzing_reeval}) based on \textit{why a seed is added to the seed queue} instead of the traditional evaluation metrics that are based on coverage and bug finding capabilities \kts{cite!}.
Specifically, we analyze the claims provided by the fuzzers and discuss if they deviate when evaluated through the lens of our methodology - \kts{remove this sentence?}.

Our benchmark contains \kts{XX} programs that are part of FuzzBench~\cite{fuzzbench}.
In total, FuzzBench contains 28 programs out of which six are bug-based benchmark programs and the rest 22 are bug-based benchmark programs. 
Evaluating on all the 22 programs is computationally intensive: according to Schloegel et al., fuzzers in top conferences are evaluated on average using 8.9 programs \cite{schloegel2024sok}.
To avoid biasing in program selection, we comprise our benchmark based on the previous fuzzing papers that uses FuzzBench in their evaluation \kts{which ones and explain why others are not part of the benchmark?}.

\kts{Why did we only consider power schedule and search strategy improvements and why not mutations too - cite Marcel's work on mutations interactions to argue?}

\subsection{AFLFast}

\aflfast introduces six power schedules and a search strategy on top of \aflone \cite{aflfast}.
According to the authors, \aflfast's power schedules assign more energy to the seeds that executes quickly, covers more code, and is generated later in the fuzzing process;
\aflfast's search strategy prioritizes seeds on how rarely they've been selected from the seed queue and how infrequently their paths have been fuzzed.
To evaluate \aflfast, we use \aflb as the baseline: \aflb has no notion of power schedule and it simply uses round-robin queue order to select seeds.

\subsubsection{Fast vs. \afl}
\textit{Fast} is the default power schedule in \aflfast.
Hence, we first evaluate it against the baseline fuzzer, \aflb.

\subsubsection{Multiple Power Schedules}
BÃ¶hme et al., showed that \textit{Fast} power schedule outperforms other power schedules in terms of number of crashes found followed by \textit{Explore} and \textit{COE} (cut-off exploration).
In this section, we compare these three power schedules in \aflfast using our methodology and discuss the deviating results.

\subsection{EcoFuzz}

\ecofuzz \cite{eco_fuzz} modifies both the power schedule and the search strategy on top of \afl. 
The authors claim that the search strategy selects next seed based on the reward probability and the power schedule aims at assigning energy based on the estimated number of mutation with the goal to reduce wasted mutations.

\subsection{FairFuzz}

\subsection{\aflpp: Explore vs. Exploit}